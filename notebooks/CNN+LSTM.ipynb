{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllScratch without vgg 50.467 13mb fifth model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Gx32RFZ4cCml",
        "colab_type": "code",
        "outputId": "ba3acc29-9a20-485f-f2cb-22182c107c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d8HRZu8_cdKo",
        "colab_type": "code",
        "outputId": "5aea4175-e2d5-41d1-d1e6-71117d7443cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'AllScratch -  BEST.ipynb'       im3.png\n",
            "'AllScratch -  BESTonpc.ipynb'   im4.png\n",
            " AllScratch.ipynb                im5.png\n",
            " AllScratchV2.ipynb              im6.png\n",
            " AllScratchV3.ipynb              image_dict.pickle\n",
            " checkP.cp                       image_dict_V2_without_VGG.pickle\n",
            " \u001b[0m\u001b[01;34mcheckpointsV2\u001b[0m/                  imagenp.npy\n",
            " embedding_matrix.pickle         label_encoder.pickle\n",
            " firstmodel.h5                   secondmodel.h5\n",
            " fourthmodel.h5                  thirdmodel.h5\n",
            " GGAllScratchV2.ipynb            \u001b[01;34mTraining\u001b[0m/\n",
            " ggcheckP.cp                     Training.zip\n",
            " glove.6B.200d.txt               vqa_model0.py\n",
            " im1.png                         word_index.pickle\n",
            " im2.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5tAfZUJ0cc_C",
        "colab_type": "code",
        "outputId": "724ee436-a9ad-4c4e-eee1-44be948f0735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/'\n",
            "/content/gdrive/My Drive/NNFL Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vd4jBOWkccrV",
        "colab_type": "code",
        "outputId": "0bae3936-f1a0-453e-a0d4-396392b5db55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'AllScratch -  BEST.ipynb'       im3.png\n",
            "'AllScratch -  BESTonpc.ipynb'   im4.png\n",
            " AllScratch.ipynb                im5.png\n",
            " AllScratchV2.ipynb              im6.png\n",
            " AllScratchV3.ipynb              image_dict.pickle\n",
            " checkP.cp                       image_dict_V2_without_VGG.pickle\n",
            " \u001b[0m\u001b[01;34mcheckpointsV2\u001b[0m/                  imagenp.npy\n",
            " embedding_matrix.pickle         label_encoder.pickle\n",
            " firstmodel.h5                   secondmodel.h5\n",
            " fourthmodel.h5                  thirdmodel.h5\n",
            " GGAllScratchV2.ipynb            \u001b[01;34mTraining\u001b[0m/\n",
            " ggcheckP.cp                     Training.zip\n",
            " glove.6B.200d.txt               vqa_model0.py\n",
            " im1.png                         word_index.pickle\n",
            " im2.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wdzWU3Ckccg-",
        "colab_type": "code",
        "outputId": "ec067d59-6ae8-4012-828a-69d914b8b5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "cd My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'My Drive'\n",
            "/content/gdrive/My Drive/NNFL Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eO71ukmccvmN",
        "colab_type": "code",
        "outputId": "97191901-f130-4b09-a01e-0e400ca485ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'AllScratch -  BEST.ipynb'       im3.png\n",
            "'AllScratch -  BESTonpc.ipynb'   im4.png\n",
            " AllScratch.ipynb                im5.png\n",
            " AllScratchV2.ipynb              im6.png\n",
            " AllScratchV3.ipynb              image_dict.pickle\n",
            " checkP.cp                       image_dict_V2_without_VGG.pickle\n",
            " \u001b[0m\u001b[01;34mcheckpointsV2\u001b[0m/                  imagenp.npy\n",
            " embedding_matrix.pickle         label_encoder.pickle\n",
            " firstmodel.h5                   secondmodel.h5\n",
            " fourthmodel.h5                  thirdmodel.h5\n",
            " GGAllScratchV2.ipynb            \u001b[01;34mTraining\u001b[0m/\n",
            " ggcheckP.cp                     Training.zip\n",
            " glove.6B.200d.txt               vqa_model0.py\n",
            " im1.png                         word_index.pickle\n",
            " im2.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "luFIt6WDc2xT",
        "colab_type": "code",
        "outputId": "b409acb7-571f-40d6-92dd-bb2d7257e37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "cd NNFL\\ Project"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'NNFL Project'\n",
            "/content/gdrive/My Drive/NNFL Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0sZa51jJc2kk",
        "colab_type": "code",
        "outputId": "61496258-b17a-4393-d8a5-c7a5f6a80d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'AllScratch -  BEST.ipynb'       im3.png\n",
            "'AllScratch -  BESTonpc.ipynb'   im4.png\n",
            " AllScratch.ipynb                im5.png\n",
            " AllScratchV2.ipynb              im6.png\n",
            " AllScratchV3.ipynb              image_dict.pickle\n",
            " checkP.cp                       image_dict_V2_without_VGG.pickle\n",
            " \u001b[0m\u001b[01;34mcheckpointsV2\u001b[0m/                  imagenp.npy\n",
            " embedding_matrix.pickle         label_encoder.pickle\n",
            " firstmodel.h5                   secondmodel.h5\n",
            " fourthmodel.h5                  thirdmodel.h5\n",
            " GGAllScratchV2.ipynb            \u001b[01;34mTraining\u001b[0m/\n",
            " ggcheckP.cp                     Training.zip\n",
            " glove.6B.200d.txt               vqa_model0.py\n",
            " im1.png                         word_index.pickle\n",
            " im2.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QIMsk-A9b0p-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CREATING IMAGE PRE-MODEL (4096 OUTPUT) ###"
      ]
    },
    {
      "metadata": {
        "id": "8vmU4BnCb0qR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# base_model = VGG16()\n",
        "# # print(base_model.summary())\n",
        "\n",
        "# # get output of penultimate layer\n",
        "# from keras.models import Model\n",
        "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uoXJwlFMb0qz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.image import load_img\n",
        "# from keras.preprocessing.image import img_to_array\n",
        "# from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# # # load an image from file\n",
        "# # image = load_img('Coffee-Mug.jpg', target_size=(224, 224))\n",
        "# # # convert the image pixels to a numpy array\n",
        "# # image = img_to_array(image)\n",
        "# # # reshape data for the model\n",
        "# # image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# # # prepare the image for the VGG model\n",
        "# # image = preprocess_input(image)\n",
        "\n",
        "# # # predict the probability across all output classes\n",
        "# # yhat = model.predict(image)\n",
        "# # print(yhat.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P7cbYqXqb0rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MAKE IMAGE DATA DICT ###\n",
        "##### key: image name\n",
        "##### value: vector of shape [1,4096], output of VGGNet"
      ]
    },
    {
      "metadata": {
        "id": "Ua5zAKxVb0rO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# imagedictv2: Images of size 100x100\n",
        "\n",
        "if(os.path.isfile('image_dict_V2_without_VGG.pickle')):\n",
        "    with open('image_dict_V2_without_VGG.pickle', 'rb') as handle:\n",
        "        image_dict_V2 = pickle.load(handle)\n",
        "        \n",
        "else:\n",
        "  folder = 'Training/images/'\n",
        "\n",
        "  image_dict_V2 = {}        \n",
        "\n",
        "  skipped = []\n",
        "  i=0\n",
        "\n",
        "  for filename in os.listdir(folder):\n",
        "      if(i%50 == 0):\n",
        "          print(\"{} images processed\".format(i))\n",
        "      i+=1\n",
        "      if filename in image_dict_V2:\n",
        "          # print (\"already in dict - moving on\")\n",
        "          continue\n",
        "      try:\n",
        "          # load an image from file\n",
        "          image = cv2.imread(os.path.join(folder, filename))\n",
        "      except:\n",
        "          print(\"Error reading file: {}!!!\".format(filename))\n",
        "          skipped.append(filename)\n",
        "          continue\n",
        "      if image is not None:\n",
        "          resized_image = cv2.resize(image, (100, 100)) \n",
        "          image_dict_V2[filename] = resized_image\n",
        "      else:\n",
        "          skipped.append(filename)\n",
        "\n",
        "  print(\"{} files skipped:\".format(len(skipped)))\n",
        "  for f in skipped:\n",
        "      print(\"    {}\".format(f))\n",
        "  print(\"dict created\")\n",
        "\n",
        "\n",
        "  print(\"Saving image_dict_V2_without_VGG.pickle\")\n",
        "  with open('image_dict_V2_without_VGG.pickle', 'wb') as handle:\n",
        "      pickle.dump(image_dict_V2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "      print('image_dict_V2_without_VGG.pickle saved')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D97j3RzCcmO4",
        "colab_type": "code",
        "outputId": "3a1bdf61-e997-45a7-fe92-ecd0fc39c8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "image_dict_V2['CLEVR_new_014001.png'].shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "4YYt0Ve_OhQ3",
        "colab_type": "code",
        "outputId": "e2b5ea92-b87f-4977-ea47-5d3acdd8ffb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "cell_type": "code",
      "source": [
        "image_dict_V2['CLEVR_new_014001.png']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[104, 105, 105],\n",
              "        [104, 105, 105],\n",
              "        [103, 103, 104],\n",
              "        ...,\n",
              "        [102, 102, 102],\n",
              "        [102, 102, 102],\n",
              "        [ 99,  99,  99]],\n",
              "\n",
              "       [[106, 106, 106],\n",
              "        [102, 103, 103],\n",
              "        [103, 104, 104],\n",
              "        ...,\n",
              "        [104, 104, 104],\n",
              "        [ 98,  98,  98],\n",
              "        [101, 101, 101]],\n",
              "\n",
              "       [[101, 102, 102],\n",
              "        [103, 103, 103],\n",
              "        [106, 106, 106],\n",
              "        ...,\n",
              "        [104, 104, 104],\n",
              "        [100, 100, 100],\n",
              "        [104, 104, 104]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[114, 115, 116],\n",
              "        [116, 116, 117],\n",
              "        [117, 118, 118],\n",
              "        ...,\n",
              "        [149, 154, 157],\n",
              "        [148, 153, 156],\n",
              "        [148, 153, 156]],\n",
              "\n",
              "       [[114, 115, 116],\n",
              "        [115, 117, 117],\n",
              "        [116, 116, 117],\n",
              "        ...,\n",
              "        [148, 153, 156],\n",
              "        [150, 155, 157],\n",
              "        [148, 153, 156]],\n",
              "\n",
              "       [[114, 116, 116],\n",
              "        [114, 115, 115],\n",
              "        [115, 116, 117],\n",
              "        ...,\n",
              "        [148, 154, 157],\n",
              "        [148, 154, 156],\n",
              "        [148, 153, 156]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "ei2YmSZnNjyh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_dict=image_dict_V2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPZd-o-cOUrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for img in image_dict.keys():\n",
        "  image_dict[img] = image_dict[img]/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbKLrVTfOfhZ",
        "colab_type": "code",
        "outputId": "005d202c-b252-44dd-d4a4-c19823dc6a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "cell_type": "code",
      "source": [
        "image_dict['CLEVR_new_014001.png']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.40784314, 0.41176471, 0.41176471],\n",
              "        [0.40784314, 0.41176471, 0.41176471],\n",
              "        [0.40392157, 0.40392157, 0.40784314],\n",
              "        ...,\n",
              "        [0.4       , 0.4       , 0.4       ],\n",
              "        [0.4       , 0.4       , 0.4       ],\n",
              "        [0.38823529, 0.38823529, 0.38823529]],\n",
              "\n",
              "       [[0.41568627, 0.41568627, 0.41568627],\n",
              "        [0.4       , 0.40392157, 0.40392157],\n",
              "        [0.40392157, 0.40784314, 0.40784314],\n",
              "        ...,\n",
              "        [0.40784314, 0.40784314, 0.40784314],\n",
              "        [0.38431373, 0.38431373, 0.38431373],\n",
              "        [0.39607843, 0.39607843, 0.39607843]],\n",
              "\n",
              "       [[0.39607843, 0.4       , 0.4       ],\n",
              "        [0.40392157, 0.40392157, 0.40392157],\n",
              "        [0.41568627, 0.41568627, 0.41568627],\n",
              "        ...,\n",
              "        [0.40784314, 0.40784314, 0.40784314],\n",
              "        [0.39215686, 0.39215686, 0.39215686],\n",
              "        [0.40784314, 0.40784314, 0.40784314]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.44705882, 0.45098039, 0.45490196],\n",
              "        [0.45490196, 0.45490196, 0.45882353],\n",
              "        [0.45882353, 0.4627451 , 0.4627451 ],\n",
              "        ...,\n",
              "        [0.58431373, 0.60392157, 0.61568627],\n",
              "        [0.58039216, 0.6       , 0.61176471],\n",
              "        [0.58039216, 0.6       , 0.61176471]],\n",
              "\n",
              "       [[0.44705882, 0.45098039, 0.45490196],\n",
              "        [0.45098039, 0.45882353, 0.45882353],\n",
              "        [0.45490196, 0.45490196, 0.45882353],\n",
              "        ...,\n",
              "        [0.58039216, 0.6       , 0.61176471],\n",
              "        [0.58823529, 0.60784314, 0.61568627],\n",
              "        [0.58039216, 0.6       , 0.61176471]],\n",
              "\n",
              "       [[0.44705882, 0.45490196, 0.45490196],\n",
              "        [0.44705882, 0.45098039, 0.45098039],\n",
              "        [0.45098039, 0.45490196, 0.45882353],\n",
              "        ...,\n",
              "        [0.58039216, 0.60392157, 0.61568627],\n",
              "        [0.58039216, 0.60392157, 0.61176471],\n",
              "        [0.58039216, 0.6       , 0.61176471]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "IyPHry89b0rg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Save dict pickle\n",
        "# import pickle\n",
        "\n",
        "# with open('image_dict.pickle', 'wb') as handle:\n",
        "#     pickle.dump(images, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "#     print('image_dict.pickle saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJzDSB7Ob0ru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### COMBINING MODELS ###"
      ]
    },
    {
      "metadata": {
        "id": "qIMeF19ob0r0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply\n",
        "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "\n",
        "# def img_model(dropout_rate):\n",
        "#     print(\"Creating image model...\")\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
        "#     return model\n",
        "\n",
        "# def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
        "#     print(\"Creating text model...\")\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(num_words, embedding_dim, \n",
        "#         weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
        "#     model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
        "#     model.add(Dropout(dropout_rate))\n",
        "#     model.add(LSTM(units=512, return_sequences=False))\n",
        "#     model.add(Dropout(dropout_rate))\n",
        "#     model.add(Dense(1024, activation='tanh'))\n",
        "#     return model\n",
        "\n",
        "# def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
        "#     vgg_model = img_model(dropout_rate)\n",
        "#     lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
        "#     print(\"Merging final model...\")\n",
        "#     fc_model = Sequential()\n",
        "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
        "#     fc_model.add(Multiply([vgg_model, lstm_model]))\n",
        "#     fc_model.add(Dropout(dropout_rate))\n",
        "#     fc_model.add(Dense(1000, activation='tanh'))\n",
        "#     fc_model.add(Dropout(dropout_rate))\n",
        "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
        "#     fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy'])\n",
        "#     return fc_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fLafkcob0r-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CREATE QUESTION DATA ###"
      ]
    },
    {
      "metadata": {
        "id": "u3mm5ASDb0sI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create 2 lists\n",
        "# one stores image ids\n",
        "# one stores question\n",
        "\n",
        "imageNamesX  = []\n",
        "questionsNLX = []\n",
        "answers = []\n",
        "\n",
        "import json\n",
        "QAs = json.load(open(\"Training/Quest_Answers.json\", 'r'))['quest_answers']\n",
        "\n",
        "for QA in QAs:\n",
        "    img_name = QA[\"Image\"]+\".png\"\n",
        "    ques = QA[\"Question\"]\n",
        "    ans = QA[\"Answer\"]\n",
        "    \n",
        "    if img_name not in image_dict:\n",
        "        print(\"Skipping {} - not found in dict\".format(img))\n",
        "        continue\n",
        "    \n",
        "    imageNamesX.append(img_name)\n",
        "    questionsNLX.append(ques)\n",
        "    answers.append(ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ei2YErUhb0sm",
        "colab_type": "code",
        "outputId": "ed440797-7171-4ec8-f24a-18c8241879f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "### NOW do word embeddings for questions\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "num_words = 81\n",
        "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(questionsNLX)\n",
        "questionsX = tokenizer.texts_to_sequences(questionsNLX)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "max_length_of_text = 200\n",
        "questionsX = pad_sequences(questionsX, maxlen=max_length_of_text)\n",
        "\n",
        "print(\"Saving word_index.pickle\")\n",
        "import pickle\n",
        "with open('word_index.pickle', 'wb') as handle:\n",
        "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(\"Saved.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 unique tokens.\n",
            "Saving word_index.pickle\n",
            "Saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nVxDXfWWb0s7",
        "colab_type": "code",
        "outputId": "8a9054bf-c1bc-4710-9751-78d145552440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# vector embeddings\n",
        "\n",
        "embeddings_index = {}\n",
        "    \n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "embedding_matrix = None\n",
        "if(os.path.isfile('embedding_matrix.pickle')):\n",
        "    print(\">> Embedding Matrix Pickle found...\")\n",
        "    with open('embedding_matrix.pickle', 'rb') as handle:\n",
        "        embedding_matrix = pickle.load(handle)\n",
        "    print(\">>> loaded!\")\n",
        "else:\n",
        "    f = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
        "\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    with open('embedding_matrix.pickle', 'wb') as handle:\n",
        "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print('embedding_matrix.pickle saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Embedding Matrix Pickle found...\n",
            ">>> loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kAPBVfODb0tX",
        "colab_type": "code",
        "outputId": "7aeae5b6-2c58-4b7e-d516-7c01f176a1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "# One hot encode answers\n",
        "\n",
        "## ONE HOT\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "data = answers\n",
        "values = array(data)\n",
        "# print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "# print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "Y = onehot_encoded\n",
        "\n",
        "print(\"Saving label_encoder.pickle\")\n",
        "import pickle\n",
        "with open('label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print('label_encoder.pickle saved')\n",
        "\n",
        "# print(onehot_encoded)\n",
        "# invert first example\n",
        "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "# print(inverted)\n",
        "\n",
        "def decode_predictions(label_encoder, predictions):\n",
        "    texts = []\n",
        "    for p in predictions:\n",
        "        text = label_encoder.inverse_transform(argmax(p))\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "label_encoder.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving label_encoder.pickle\n",
            "label_encoder.pickle saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', 'False', 'True',\n",
              "       'blue', 'brown', 'cube', 'cyan', 'cylinder', 'gray', 'green',\n",
              "       'large', 'metal', 'purple', 'red', 'rubber', 'small', 'sphere',\n",
              "       'yellow'], dtype='<U21')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "7gwJ6iVo8LMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Split quesionsX\n",
        "from sklearn.model_selection import train_test_split\n",
        "imageNamesX_train, imageNamesX_test, questionsX_train, questionsX_test, Y_train, Y_test = train_test_split(imageNamesX, questionsX, Y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oyd0udLIb0tj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data generator ###"
      ]
    },
    {
      "metadata": {
        "id": "EW3wTiFDs3kj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Oi9ICY1Vb0to",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(image_dict, img_names, questions, labels, batch_size):\n",
        "    \n",
        "    q_ptr = 0\n",
        "    while True:\n",
        "        image_inp = []\n",
        "        q_inp = []\n",
        "        batch_labels = []\n",
        "        for i in range(batch_size):\n",
        "            if q_ptr == len(questions):\n",
        "                q_ptr = 0\n",
        "            index = q_ptr\n",
        "#             import random\n",
        "#             index= random.randint(0, len(questions)-1)\n",
        "            # print(imageNamesX[q_ptr].shape)\n",
        "            # print(questionsX[q_ptr])\n",
        "            image_inp.append(image_dict[img_names[index]])\n",
        "            q_inp.append(questions[index])\n",
        "            batch_labels.append(labels[index])\n",
        "            q_ptr+=1\n",
        "            \n",
        "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9w-mb30esbv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def generator2(image_dict, img_names, questions, labels, batch_size):\n",
        "    \n",
        "#     q_ptr = 0\n",
        "#     while True:\n",
        "#         image_inp = []\n",
        "#         q_inp = []\n",
        "#         batch_labels = []\n",
        "#         for i in range(batch_size):\n",
        "#             if q_ptr == len(questions):\n",
        "#                 q_ptr = 0\n",
        "#             index = q_ptr\n",
        "#             # print(imgsX2[q_ptr].shape)\n",
        "#             # print(quesX2_sq[q_ptr])\n",
        "#             image_inp.append(image_dict[img_names[index]])\n",
        "#             q_inp.append(questions[index])\n",
        "#             batch_labels.append(Y[index])\n",
        "#             q_ptr+=1\n",
        "#         yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCjn7TT1esHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras.models import Sequential, Model\n",
        "# from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate\n",
        "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "# import keras\n",
        "\n",
        "\n",
        "# def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
        "    \n",
        "#     print(\"Creating image model...\")\n",
        "#     img_model = Sequential()\n",
        "#     img_model.add(Dense(512, input_dim=4096, activation='relu'))\n",
        "\n",
        "#     image_input = Input(shape=(4096, ))\n",
        "#     encoded_image = img_model(image_input)\n",
        "\n",
        "#     print(\"Creating text model...\")\n",
        "#     txt_model = Sequential()\n",
        "#     txt_model.add(Embedding(num_words, embedding_dim, \n",
        "#         weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
        "#     txt_model.add(LSTM(units=128, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
        "#     txt_model.add(Dropout(dropout_rate))\n",
        "# #     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
        "# #     txt_model.add(LSTM(units=512, return_sequences=False))\n",
        "# #     txt_model.add(Dropout(dropout_rate))\n",
        "# #     txt_model.add(Dense(1024, activation='tanh'))\n",
        "    \n",
        "#     question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
        "#     embedded_question = txt_model(question_input)\n",
        "    \n",
        "#     print(txt_model.summary())\n",
        "    \n",
        "#     print(\"Merging final model...\")\n",
        "#     merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
        "#     d1  = Dense(512, activation='relu')(merged)\n",
        "#     dp1 = Dropout(dropout_rate)(d1)\n",
        "# #     d2  = Dense(1000, activation='tanh')(dp1)\n",
        "# #     dp2 = Dropout(dropout_rate)(d2)\n",
        "#     output  = Dense(num_classes, activation='softmax')(dp1)\n",
        "    \n",
        "#     vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "    \n",
        "    \n",
        "# #     fc_model = Sequential()\n",
        "# #     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
        "# #     fc_model.add(Concatenate([img_model, txt_model]))\n",
        "# #     fc_model.add(Dropout(dropout_rate))\n",
        "# #     fc_model.add(Dense(1000, activation='tanh'))\n",
        "# #     fc_model.add(Dropout(dropout_rate))\n",
        "# #     fc_model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "#     vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "#     return vqa_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmKQVqgePIkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate, Conv2D, BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "import keras\n",
        "\n",
        "\n",
        "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
        "    \n",
        "    print(\"Creating image model...\")\n",
        "    img_model = Sequential()\n",
        "    img_model.add(Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    img_model.add(BatchNormalization())\n",
        "    img_model.add(Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    img_model.add(BatchNormalization())\n",
        "    img_model.add(Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    img_model.add(BatchNormalization())\n",
        "    img_model.add(Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    img_model.add(BatchNormalization())\n",
        "    img_model.add(keras.layers.Flatten())\n",
        "    \n",
        "    image_input = Input(shape=(100, 100, 3))\n",
        "    encoded_image = img_model(image_input)\n",
        "    \n",
        "    print(img_model.summary())\n",
        "\n",
        "    print(\"Creating text model...\")\n",
        "    txt_model = Sequential()\n",
        "    txt_model.add(Embedding(num_words, embedding_dim, \n",
        "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
        "    txt_model.add(LSTM(units=128, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
        "    txt_model.add(Dropout(dropout_rate))\n",
        "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
        "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
        "#     txt_model.add(Dropout(dropout_rate))\n",
        "#     txt_model.add(Dense(1024, activation='tanh'))\n",
        "    \n",
        "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
        "    embedded_question = txt_model(question_input)\n",
        "    \n",
        "    print(txt_model.summary())\n",
        "    \n",
        "    print(\"Merging final model...\")\n",
        "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
        "    d1  = Dense(512, activation='relu')(merged)\n",
        "    dp1 = Dropout(dropout_rate)(d1)\n",
        "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
        "#     dp2 = Dropout(dropout_rate)(d2)\n",
        "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
        "    \n",
        "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "    \n",
        "    \n",
        "#     fc_model = Sequential()\n",
        "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
        "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
        "#     fc_model.add(Dropout(dropout_rate))\n",
        "#     fc_model.add(Dense(1000, activation='tanh'))\n",
        "#     fc_model.add(Dropout(dropout_rate))\n",
        "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return vqa_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yx5BNs_mer_W",
        "colab_type": "code",
        "outputId": "6c31f9c1-cefb-48e0-be89-6b62656b5bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3086
        }
      },
      "cell_type": "code",
      "source": [
        "dropout_rate=0.5\n",
        "num_classes=26\n",
        "model_weights_filename = \"weights.bkp\"\n",
        "ckpt_model_weights_filename = \"checkP.cp\"\n",
        "\n",
        "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
        "if os.path.exists(model_weights_filename):\n",
        "    print (\"Loading Weights...\")\n",
        "    model.load_weights(model_weights_filename)\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1, monitor = 'val_acc', save_best_only = True)\n",
        "\n",
        "batch_size = 128\n",
        "model.fit_generator(\n",
        "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
        "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
        "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=30, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating image model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 49, 49, 24)        672       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 49, 49, 24)        96        \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 48)        10416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 24, 24, 48)        192       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 11, 11, 48)        20784     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 11, 11, 48)        192       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 64)          27712     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 5, 5, 64)          256       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1600)              0         \n",
            "=================================================================\n",
            "Total params: 60,320\n",
            "Trainable params: 59,952\n",
            "Non-trainable params: 368\n",
            "_________________________________________________________________\n",
            "None\n",
            "Creating text model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 200)          16200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               168448    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 184,648\n",
            "Trainable params: 168,448\n",
            "Non-trainable params: 16,200\n",
            "_________________________________________________________________\n",
            "None\n",
            "Merging final model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=106.314960..., steps_per_epoch=956.834645..., callbacks=[<keras.ca..., epochs=30)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "957/956 [==============================] - 625s 653ms/step - loss: 1.3164 - acc: 0.3892 - val_loss: 0.9879 - val_acc: 0.4586\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.45860, saving model to checkP.cp\n",
            "Epoch 2/30\n",
            "957/956 [==============================] - 619s 647ms/step - loss: 1.0177 - acc: 0.4350 - val_loss: 1.0883 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.45860\n",
            "Epoch 3/30\n",
            "957/956 [==============================] - 639s 668ms/step - loss: 0.9982 - acc: 0.4415 - val_loss: 0.9932 - val_acc: 0.4267\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.45860\n",
            "Epoch 4/30\n",
            "957/956 [==============================] - 631s 659ms/step - loss: 1.0155 - acc: 0.4417 - val_loss: 0.9815 - val_acc: 0.4495\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.45860\n",
            "Epoch 5/30\n",
            "957/956 [==============================] - 639s 668ms/step - loss: 0.9808 - acc: 0.4482 - val_loss: 1.3399 - val_acc: 0.4188\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.45860\n",
            "Epoch 6/30\n",
            "957/956 [==============================] - 631s 659ms/step - loss: 0.9736 - acc: 0.4503 - val_loss: 1.0709 - val_acc: 0.4209\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.45860\n",
            "Epoch 7/30\n",
            "957/956 [==============================] - 629s 658ms/step - loss: 0.9670 - acc: 0.4553 - val_loss: 1.1632 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.45860\n",
            "Epoch 8/30\n",
            "957/956 [==============================] - 597s 624ms/step - loss: 0.9583 - acc: 0.4635 - val_loss: 0.9882 - val_acc: 0.4447\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.45860\n",
            "Epoch 9/30\n",
            "957/956 [==============================] - 577s 603ms/step - loss: 0.9534 - acc: 0.4691 - val_loss: 0.9878 - val_acc: 0.4417\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.45860\n",
            "Epoch 10/30\n",
            "957/956 [==============================] - 571s 597ms/step - loss: 0.9418 - acc: 0.4735 - val_loss: 0.9616 - val_acc: 0.4568\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.45860\n",
            "Epoch 11/30\n",
            "957/956 [==============================] - 571s 596ms/step - loss: 0.9329 - acc: 0.4836 - val_loss: 0.9989 - val_acc: 0.4538\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.45860\n",
            "Epoch 12/30\n",
            "957/956 [==============================] - 565s 590ms/step - loss: 0.9210 - acc: 0.4931 - val_loss: 0.9860 - val_acc: 0.4663\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.45860 to 0.46627, saving model to checkP.cp\n",
            "Epoch 13/30\n",
            "957/956 [==============================] - 567s 593ms/step - loss: 0.9100 - acc: 0.5037 - val_loss: 0.9770 - val_acc: 0.4720\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.46627 to 0.47196, saving model to checkP.cp\n",
            "Epoch 14/30\n",
            "957/956 [==============================] - 564s 590ms/step - loss: 0.8975 - acc: 0.5142 - val_loss: 1.1508 - val_acc: 0.4485\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.47196\n",
            "Epoch 15/30\n",
            "957/956 [==============================] - 565s 590ms/step - loss: 0.8835 - acc: 0.5277 - val_loss: 1.0000 - val_acc: 0.4558\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.47196\n",
            "Epoch 16/30\n",
            "957/956 [==============================] - 569s 595ms/step - loss: 0.8678 - acc: 0.5408 - val_loss: 1.0165 - val_acc: 0.4737\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.47196 to 0.47371, saving model to checkP.cp\n",
            "Epoch 17/30\n",
            "957/956 [==============================] - 570s 595ms/step - loss: 0.8541 - acc: 0.5508 - val_loss: 1.0727 - val_acc: 0.4618\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.47371\n",
            "Epoch 18/30\n",
            "957/956 [==============================] - 558s 583ms/step - loss: 0.8393 - acc: 0.5629 - val_loss: 1.1212 - val_acc: 0.4698\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.47371\n",
            "Epoch 19/30\n",
            "957/956 [==============================] - 581s 607ms/step - loss: 0.8255 - acc: 0.5722 - val_loss: 1.1065 - val_acc: 0.4713\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.47371\n",
            "Epoch 20/30\n",
            "957/956 [==============================] - 608s 635ms/step - loss: 0.8141 - acc: 0.5800 - val_loss: 1.1018 - val_acc: 0.4806\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.47371 to 0.48058, saving model to checkP.cp\n",
            "Epoch 21/30\n",
            "957/956 [==============================] - 596s 623ms/step - loss: 0.7975 - acc: 0.5911 - val_loss: 1.0902 - val_acc: 0.4852\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.48058 to 0.48518, saving model to checkP.cp\n",
            "Epoch 22/30\n",
            "957/956 [==============================] - 559s 584ms/step - loss: 0.7856 - acc: 0.5990 - val_loss: 1.0540 - val_acc: 0.4913\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.48518 to 0.49131, saving model to checkP.cp\n",
            "Epoch 23/30\n",
            "957/956 [==============================] - 557s 582ms/step - loss: 0.7741 - acc: 0.6078 - val_loss: 1.1420 - val_acc: 0.4706\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.49131\n",
            "Epoch 24/30\n",
            "957/956 [==============================] - 558s 583ms/step - loss: 0.7619 - acc: 0.6160 - val_loss: 1.1773 - val_acc: 0.4734\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.49131\n",
            "Epoch 25/30\n",
            "957/956 [==============================] - 554s 579ms/step - loss: 0.7529 - acc: 0.6195 - val_loss: 1.1018 - val_acc: 0.4858\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.49131\n",
            "Epoch 26/30\n",
            "957/956 [==============================] - 538s 562ms/step - loss: 0.7398 - acc: 0.6285 - val_loss: 1.1742 - val_acc: 0.4908\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.49131\n",
            "Epoch 27/30\n",
            "957/956 [==============================] - 542s 567ms/step - loss: 0.7321 - acc: 0.6333 - val_loss: 1.1220 - val_acc: 0.4945\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.49131 to 0.49452, saving model to checkP.cp\n",
            "Epoch 28/30\n",
            "957/956 [==============================] - 576s 601ms/step - loss: 0.7200 - acc: 0.6410 - val_loss: 1.1768 - val_acc: 0.5047\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.49452 to 0.50467, saving model to checkP.cp\n",
            "Epoch 29/30\n",
            "957/956 [==============================] - 579s 605ms/step - loss: 0.7091 - acc: 0.6474 - val_loss: 1.1535 - val_acc: 0.4872\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.50467\n",
            "Epoch 30/30\n",
            "957/956 [==============================] - 585s 611ms/step - loss: 0.7014 - acc: 0.6528 - val_loss: 1.1121 - val_acc: 0.4943\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.50467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f43f56b1d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "MPDVQ4PaajXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('fifthmodel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srD6YQGLHh1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model1 = load_model('.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v56iuShqH4au",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# score_acc = model1.evaluate_generator(generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),steps = len(Y_test)/batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvblgkdQMoQp",
        "colab_type": "code",
        "outputId": "56ed6f35-bee1-44f1-9481-5078831824b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# print(score_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9648685213529838, 0.46875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2o891mvXfgD",
        "colab_type": "code",
        "outputId": "26c35927-e1c4-4cc4-c474-452016b93565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
        "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
        "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=10, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=106.314960..., steps_per_epoch=956.834645..., callbacks=[<keras.ca..., epochs=10)`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "957/956 [==============================] - 509s 531ms/step - loss: 0.9348 - acc: 0.4869 - val_loss: 0.9711 - val_acc: 0.4774\n",
            "\n",
            "Epoch 00001: saving model to checkP.cp\n",
            "Epoch 2/10\n",
            "957/956 [==============================] - 508s 531ms/step - loss: 0.9254 - acc: 0.4996 - val_loss: 0.9947 - val_acc: 0.4830\n",
            "\n",
            "Epoch 00002: saving model to checkP.cp\n",
            "Epoch 3/10\n",
            "957/956 [==============================] - 512s 535ms/step - loss: 0.9119 - acc: 0.5129 - val_loss: 0.9544 - val_acc: 0.4931\n",
            "\n",
            "Epoch 00003: saving model to checkP.cp\n",
            "Epoch 4/10\n",
            "957/956 [==============================] - 512s 535ms/step - loss: 0.9046 - acc: 0.5230 - val_loss: 1.0765 - val_acc: 0.4619\n",
            "\n",
            "Epoch 00004: saving model to checkP.cp\n",
            "Epoch 5/10\n",
            "957/956 [==============================] - 511s 534ms/step - loss: 0.8834 - acc: 0.5348 - val_loss: 1.0221 - val_acc: 0.4849\n",
            "\n",
            "Epoch 00005: saving model to checkP.cp\n",
            "Epoch 6/10\n",
            "957/956 [==============================] - 507s 530ms/step - loss: 0.8703 - acc: 0.5438 - val_loss: 1.1683 - val_acc: 0.4879\n",
            "\n",
            "Epoch 00006: saving model to checkP.cp\n",
            "Epoch 7/10\n",
            "957/956 [==============================] - 513s 536ms/step - loss: 0.8567 - acc: 0.5544 - val_loss: 1.7525 - val_acc: 0.4671\n",
            "\n",
            "Epoch 00007: saving model to checkP.cp\n",
            "Epoch 8/10\n",
            "957/956 [==============================] - 513s 536ms/step - loss: 0.8419 - acc: 0.5665 - val_loss: 1.6495 - val_acc: 0.4784\n",
            "\n",
            "Epoch 00008: saving model to checkP.cp\n",
            "Epoch 9/10\n",
            "957/956 [==============================] - 510s 532ms/step - loss: 0.8302 - acc: 0.5750 - val_loss: 1.6515 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00009: saving model to checkP.cp\n",
            "Epoch 10/10\n",
            "957/956 [==============================] - 506s 528ms/step - loss: 0.8198 - acc: 0.5819 - val_loss: 1.2545 - val_acc: 0.4584\n",
            "\n",
            "Epoch 00010: saving model to checkP.cp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f55c099bf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "4G0jNgMVaC3F",
        "colab_type": "code",
        "outputId": "220fd90c-5486-4bdf-9481-dca5e77ef93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
        "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
        "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=10, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=106.314960..., steps_per_epoch=956.834645..., callbacks=[<keras.ca..., epochs=10)`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "957/956 [==============================] - 504s 527ms/step - loss: 0.8052 - acc: 0.5940 - val_loss: 1.0401 - val_acc: 0.4850\n",
            "\n",
            "Epoch 00001: saving model to checkP.cp\n",
            "Epoch 2/10\n",
            "957/956 [==============================] - 505s 528ms/step - loss: 0.7907 - acc: 0.5995 - val_loss: 1.1792 - val_acc: 0.4589\n",
            "\n",
            "Epoch 00002: saving model to checkP.cp\n",
            "Epoch 3/10\n",
            "957/956 [==============================] - 507s 530ms/step - loss: 0.7796 - acc: 0.6075 - val_loss: 1.4506 - val_acc: 0.4612\n",
            "\n",
            "Epoch 00003: saving model to checkP.cp\n",
            "Epoch 4/10\n",
            "356/956 [==========>...................] - ETA: 5:13 - loss: 0.7453 - acc: 0.6298"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AnHnrtchaE7J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}